---
title: "Final_Working_Project_2"
output: pdf_document
date: "2025-03-07"
---

# Title

### Preston O'Connor, Khoa Dao, Nick Wierzbowski

### 3/12/2025

# Introduction

Our model is representing the relation between a variety of predictor variables, including number of floors, condition and other similar qualities of a house, and the sale price of a house. Our data is obtained from OpenML but was originally from Kaggle, where it was uploaded in 2015. It includes data for homes sold between May 2014 and May 2015. In short we included in our analysis variables that in some way described the condition or quality of the house, as using a multiple linear regression model with geo-spatial elements is beyond our current scope. Sale price, aside from being the price variable used in the dataset, is also a good indicator for actual market conditions as compared to initial listings.

A question that this model addresses is the actual corollaries of house price in a city in the U.S; if house price is correlated with the quality (and thus production cost) of the product then a solution to the growing crisis of homelessness within our major cities would be to construct higher volumes of lower cost homes, which might inform policy decisions around subsidized housing and affordable housing schemes. On the other hand, if price is weakly correlated then it might suggest that actual housing costs have more to do with other factors which would inform very different policy decisions. Of course we should note that not accounting for geo-spatial, demographic and other information in this analysis is a pretty substantial omission for actual predictive capabilities, and thus our error term is going to be large. At the end of the day, the main issue is that our features are correlated with terms we don’t account for, producing a problem of endogeneity in our model. Malpezzi considers the implicit social costs and benefits of externalities in the housing market, among which are of course subsidized housing units and other schemes that aim to depress the cost of housing at the lower end, which alongside the existence of inequality may obviously skew our data. Yadavalli et al. takes into consideration a myriad of features to produce clusters of cities wherein different variables contribute the most to housing price levels. This further illustrates a reason why our model may not produce significant correlations. 

To briefly go over each package used in this project, caret has functions used for streamlining predictive model creation. Tidyverse has a lot of general data science related functions. Tidymodels uses a lot of the same principles of tidyverse but has some more utility for modeling purposes. Leaps was used for subset selection. Car is used for linear regression analysis, in particular we used it for its Variance Inflation Factor model. Dplyr is used for data manipulation. MASS is an applied statistics package that we used for a stepwise function. Glmnet fits linear models with penalized maximum likelihood. We used foreign to read the .arff data file. GGally is an extension to ggplot2 that adds some functionality. Corrr is a package for easily creating and analyzing correlation matrices. Ggplot2 is a graphing system. Ggcorrplot enables easy visualization of correlation matrices.

Not too sure on results rn.


# Note: We are modeling to see if there is a good multivariate regression model to predict the prices of the houses in Seattle

```{r}
#Call this section of code to install all of the libraries on your device
#install.packages(c("caret", "ggplot2", "MASS", "car", "dplyr", "tibble", "foreign","reshape2"))
#Refernce Library calls to which data sets will be implemented and used

library(caret)
library(tidyverse)
library(tidymodels)
library(leaps)
library(car)  # for the VIF Model
library(dplyr)
library(MASS) # used for the stepwise function and see how the data is being implemented
library(glmnet)
library(foreign) # used to read the arff file
library(GGally)
library(corrr)
library(ggplot2)
library(ggcorrplot)
```

# Data Description

## Data Resource

The dataset used in this study is the "House Sales in King County, USA" dataset, obtained from <https://www.openml.org/search?type=data&status=active&id=42635&sort=runs>. It contains detailed information about house sales in King County, Washington, between May 2014 and May 2015.

## Data Structure

The dataset consists of 21,613 rows and 21 columns, including the target variable (price) and several predictive features.

## Variables Description

The dataset includes the following key variables:

**Dependent Variable** (Target Variable):

-   `price` (*Numeric*): The price at which the house was sold (in USD).

**Independent Variables** (Predictors):

-   `id` (*Categorical*): A unique identifier for each house.

-   `date` (*Date*): The date the house was sold.

-   `bedrooms` (*Numeric*): Number of bedrooms in the house.

-   `bathrooms` (*Numeric*): Number of bathrooms (includes fractional values for half-bathrooms).

-   `sqft_living` (*Numeric*): The total square footage of living space.

-   `sqft_lot` (*Numeric*): The total square footage of the lot.

-   `floors` (*Numeric*): The number of floors in the house.

-   `waterfront` (*Binary*): Whether the house has a waterfront view (`1 = yes`, `0 = no`).

-   `view` (*Ordinal*): A rating of the house’s view (`0` to `4`).

-   `condition` (*Ordinal*): The overall condition of the house (`1` to `5`).

-   `grade` (*Ordinal*): The quality of construction and design (`1` to `13`).

-   `sqft_above` (*Numeric*): Square footage of house excluding basement.

-   `sqft_basement` (*Numeric*): Square footage of the basement.

-   `yr_built` (*Numeric*): The year the house was built.

-   `yr_renovated` (*Numeric*): The year of the last renovation (`0` if never renovated).

-   `zipcode` (*Categorical*): The ZIP code of the house.

-   `lat` (*Numeric*): The latitude of the house.

-   `long` (*Numeric*): The longitude of the house.

-   `sqft_living15` (*Numeric*): Average square footage of living space of the 15 closest houses.

-   `sqft_lot15` (*Numeric*): Average square footage of lots of the 15 closest houses.

## Import working data set

```{r}
# load and display the data set we are working with
data <- read.arff("house_sales_reduced.arff")
head(data)
```

## Clean Data

### Remove outliers that are outside 3 standard deviation

```{r}
# filter out data outside of 3 standard deviations
# come back here to see if we want to remove the waterfront and View booleans
# Deletes the row with outlier

cleaned <- data %>% 
  dplyr::select(-sqft_living15, -sqft_lot15, -id, -attribute_0, -lat, -long, -zipcode) %>% 
  mutate(ln_price = log(price)) %>%
  filter(across(where(is.numeric), ~ abs(. - mean(.)) <= 3 * sd(.))) # originally was 3
head(cleaned)
```

## Data Visualization

### Histogram of house price

```{r}
ggplot(cleaned, aes(x = price)) +
  geom_histogram(fill = "blue", bins = 30) +
  theme_minimal() +
  ggtitle("Distribution of House Prices")
```

### Histogram of ln(house price)

```{r}
ggplot(cleaned, aes(x = ln_price)) +
  geom_histogram(fill = "blue", bins = 30) +
  theme_minimal() +
  ggtitle("Distribution of ln(House Prices)")
```

The original histogram of house prices is right-skewed, meaning most homes have lower prices, while a few expensive houses create a long right tail. This skewness violates normality assumptions. Taking the log transformation makes it more symmetric and reduces the influence of extreme high-priced houses, which improves model interpretability and accuracy. \### Boxplot of ln(house price)

```{r}
boxplot(cleaned$ln_price, main = "Boxplot of ln(House Prices)", col = "blue", ylab = "ln_price")
```

Since the boxplot shows some outliers outside the interquartile range, we will remove these outliers. \### Boxplot of ln(house price) after removing outliers

```{r}
# Find the IQR range
IQR_of_ln_price <- IQR(cleaned$ln_price, na.rm = TRUE)
ln_price_lower <- quantile(cleaned$ln_price, 0.25, na.rm = TRUE) - 1.5 * IQR_of_ln_price
ln_price_upper <- quantile(cleaned$ln_price, 0.75, na.rm = TRUE) + 1.5 * IQR_of_ln_price

# Continue to filter any of the outliers outside IQR
cleaned <- cleaned %>%
  filter(ln_price >= ln_price_lower & ln_price <= ln_price_upper)

boxplot(cleaned$ln_price, main = "Boxplot of House Prices", col = "blue", ylab = "ln_price")
```

The boxplot of log-transformed house prices (ln_price) displays a symmetrical distribution with a compact interquartile range, indicating reduced variability. There are fewer outliers compared to the original price distribution, confirming that the log transformation effectively reduces skewness and normalizes the data.

### Correlation heatmap

```{r}
zero_variance_vars <- nearZeroVar(cleaned, saveMetrics = TRUE)
print(zero_variance_vars)

# Drop near-zero variance and zero-variance variables to draw correlation matrix
cleaned_filtered <- cleaned %>%
  dplyr::select(-waterfront, -yr_renovated, -sqft_basement, -view, -price)

cor_matrix <- cor(cleaned_filtered %>% select_if(is.numeric))
ggcorrplot::ggcorrplot(cor_matrix, hc.order = TRUE, type = "lower", lab = TRUE)
```

The correlation heatmap shows the relationships between different features in the dataset. `ln_price` (log-transformed house price) is strongly correlated with `sqft_living` (0.64), `grade` (0.72), and `bathrooms` (0.62), indicating that larger and higher-quality homes tend to be more expensive. `sqft_living` and `sqft_above` have a very high correlation (0.86), suggesting possible multicollinearity. `yr_built` and `condition` show weak or negative correlations with other features, implying they have less impact on price.

## Implement another helper for outlier filtering???

```{r}

multi_reg_model <- lm(ln_price ~ . -price, data = cleaned)  

stepwise_model <- stepAIC(multi_reg_model, direction = "backward", trace = FALSE)

summary(stepwise_model)
vif(stepwise_model) # delete
```

# end of research and messing around data

# Analysis

### Selecting Variables for the Multivariate Regression Model

```{r}
multi_reg_model <- lm(ln_price ~ . -price, data = cleaned)  

stepwise_model <- stepAIC(multi_reg_model, direction = "backward", trace = FALSE)

summary(stepwise_model)
vif(stepwise_model) # delete
```

### Outline Potential Model Issues

```{r}
par(mfrow = c(2, 2))
plot(multi_reg_model)
```

Most of our value should sit in between the 3 and the negative 3

### Implementing Cook's Distance to find outlier points

```{r}
cooks_d <- cooks.distance(stepwise_model)

# Number of observations
n <- length(cooks_d)
threshold <- 4 / n  # Common threshold for identifying outliers

# Identify outliers based on Cook's Distance threshold
outliers <- which(cooks_d > threshold)

# Retrieve only the outlier data rows from the cleaned dataset
outlier_data <- cleaned[outliers, ]

# Create a summary of the outliers with their Cook's Distance values
outliers_summary <- data.frame(Cook_Distance = cooks_d[outliers], outlier_data)

# Count the number of outliers
outlier_count <- length(outliers)

# Print out the count of outliers
cat("Number of outliers found:", outlier_count, "\n")

view(outliers_summary)
nrow(outliers_summary)
# Print the outlier summary
# print(outliers_summary)
```

#### Cook's Distance to Find influential points

```{r}
cooksD <- cooks.distance(stepwise_model)
plot(cooksD,type="b",pch=18,col="red")
influential <- cooksD[(cooksD > (3 * mean(cooksD, na.rm = TRUE)))]
```

Take note of the number of outliers in the cook's distance model we are implementing

#### Identify and Remove Influential Points

```{r}
# Define a threshold for influential points
threshold <- 3 * mean(cooksD, na.rm = TRUE)

# Identify influential points
influential <- which(cooksD > threshold)

cleaned_without_influential <- cleaned[-influential, ]

```

#### Refited Model Minus Outliers

```{r}
model2 <- lm(ln_price ~ sqft_living + bathrooms + grade + floors + view + condition + yr_built, data = cleaned_without_influential)

summary(model2)
```

#### Updated new Diagnositc Plot

```{r}
par(mfrow = c(2, 2))
plot(model2)
```

### Variance Inflation Factor

```{r}
vif(stepwise_model)
```

-   There are High VIFs with `sqft_living` and `sqft_above`, which both have a VIFs that are higher than 5. Therefore, there is some degree of clolinearity between these variables. Below in the collinearity, the two variables are highly correlated with each other at 0.86. which may inturn be causing multicolinearity in the model.

-   `bathrooms` and `floors` also have a colinearity greater then 2, meaning we have a moderate multicollinearity. However, they are not as big of a concern as the previous

### Colinearity

```{r}
cleaned_without_influential %>% 
  dplyr::select(ln_price, bedrooms, bathrooms, sqft_living, sqft_lot, floors, view, condition, grade, sqft_above, yr_built) %>% 
  cor() %>% 
  round(2)
```

-   From here we see that there are strong correlations between sqft_living and sqft_above (0.86), sqft_living and bathrooms(0.71), and sqft_living and grade(0.72). Given that sqft_living has the highest VIF, it is likely driving much of the collinearity in the model.

### Removal of `sqft_living` on the Model

```{r}
model <- lm(ln_price ~ sqft_above + bathrooms + grade + floors + view + condition + yr_built, data = cleaned_without_influential)
summary(model)
vif(model)
```

### Removal of `sqft_above` on the Model

```{r}
final_regression_model <- lm(ln_price ~ sqft_living + grade + bathrooms+ floors + view + condition + yr_built, data = cleaned_without_influential)
summary(final_regression_model )
vif(final_regression_model )
```

-   From an hand out from the Northwoods Realtors, the square footage of the home's living space is an area that includes all finished, heated areas across all floors, encompassing the second floor if it meets this criteria. Given the criteria of `sqft_living` encompasses the `above_sqft` due to Washington and Seatle building, and the adjusted R-sqaure only having a minor decrease in the original Adjusted R Sqaured its safe to remove the columns and continue the models

### F-test

```{r}
results <- anova(final_regression_model)
print(results)
```

We remove `floors` because its p-value of 0.044 indicates low statistical significance, and its F-value of roughly 4.05 shows it adds little explanatory power to the model. Removing it results in a minor drop in adjusted R-squared (from 0.6338 to 0.6243), which suggests that the model becomes slightly less predictive but simpler overall, making the trade-off worthwhile as the loss in accuracy is minimal.

### The Final Regression model

```{r}
final_regression_model <- lm(ln_price ~ sqft_living + grade + bathrooms+ view + condition + yr_built, data = cleaned_without_influential)
summary(final_regression_model )
vif(final_regression_model )
```

Question(Did we ever go over using Ridge and Lasso in Regression models)

### Repeated K fold cross-validation

```{r}
set.seed(125)

train_control <- trainControl(method = "repeatedcv", 
                               number = 10,       # Number of folds (K)
                               repeats = 3)       # Number of repetitions

model <- train(ln_price ~ sqft_living + bathrooms + grade + view
               + condition + yr_built, 
               data = cleaned_without_influential, 
               method = "lm", 
               trControl = train_control)

final_model <- model$finalModel       
residuals_final <- residuals(final_model)

print(model)
```

### Q-Q Residuals

```{r}
# Create Q-Q plot of the residuals
qqnorm(residuals_final)                  
qqline(residuals_final, col = "red")    
```

# Ridge Regression evaluation of the Model

```{r}
# Normalizing the data set
X <- cleaned_without_influential[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", 
                                     "floors", "view", "condition", "grade", "sqft_above", "yr_built")]
Y <- cleaned_without_influential$ln_price

set.seed(1)  # For reproducibility
trainIndex <- createDataPartition(Y, p = 0.7, list = FALSE)

# Split data
X_train <- X[trainIndex, ]
X_test  <- X[-trainIndex, ]
Y_train <- Y[trainIndex]
Y_test  <- Y[-trainIndex]

cols_to_scale <- c("sqft_living", "sqft_lot", "sqft_above", "yr_built")

# Standardize only relevant columns
preProc <- preProcess(X_train[, cols_to_scale], method = c("center", "scale"))
X_train[, cols_to_scale] <- predict(preProc, X_train[, cols_to_scale])
X_test[, cols_to_scale] <- predict(preProc, X_test[, cols_to_scale])

#into a matrix format
X_train_mat <- as.matrix(X_train)
X_test_mat  <- as.matrix(X_test)

lambdas <- 10^seq(2, -3, by = -0.1)

# Train Ridge Regression with Cross-Validation to find the best lambda
ridge_cv <- cv.glmnet(X_train_mat, Y_train, alpha = 0, lambda = lambdas)

# Get the best lambda
best_lambda_ridge <- ridge_cv$lambda.min

# Train Ridge Regression with the best lambda
ridge_model <- glmnet(X_train_mat, Y_train, alpha = 0, lambda = best_lambda_ridge)

```

## Model Predictions and Evalueation

```{r}
ridge_preds_train <- predict(ridge_model, newx = X_train_mat)
ridge_preds_test  <- predict(ridge_model, newx = X_test_mat)

```

## Ridge Regression Model

```{r}
eval_metrics <- function(y_true, y_pred) {
  SSE <- sum((y_pred - y_true)^2)
  SST <- sum((y_true - mean(y_true))^2)
  
  r2 <- 1 - SSE / SST
  RMSE <- sqrt(mean((y_pred - y_true)^2))
  
  return(list(RMSE = RMSE, R2 = r2))
}

# Evaluate Ridge Model
ridge_train_results <- eval_metrics(Y_train, ridge_preds_train)
ridge_test_results <- eval_metrics(Y_test, ridge_preds_test)

print(paste("Ridge Regression - Train RMSE:", ridge_train_results$RMSE, "R²:", ridge_train_results$R2))
print(paste("Ridge Regression - Test RMSE:", ridge_test_results$RMSE, "R²:", ridge_test_results$R2))

```

## Ridge Coefficients

```{r}
ridge_coeffs <- coef(ridge_model)
ridge_df <- data.frame(Feature = rownames(ridge_coeffs), Coefficient = as.vector(ridge_coeffs))

# Display only important (nonzero) coefficients
ridge_df <- ridge_df[ridge_df$Coefficient != 0, ]
print(ridge_df)
```

## Model Visual of Ridge Regression

```{r}
plot(Y_test, ridge_preds_test, main = "Actual vs. Predicted Prices (Ridge Regression)",
     xlab = "Actual Prices", ylab = "Predicted Prices", col = "blue", pch = 16)
abline(0, 1, col = "red", lwd = 2)
```

## Lasso Regression Evaluation

```{r}
# Define lambda sequence for regularization
lambdas <- 10^seq(2, -3, by = -0.1)

# Train Lasso Regression with Cross-Validation to find best lambda
lasso_cv <- cv.glmnet(X_train_mat, Y_train, alpha = 1, lambda = lambdas)

# Get the best lambda
best_lambda_lasso <- lasso_cv$lambda.min

# Train final Lasso model with best lambda
lasso_model <- glmnet(X_train_mat, Y_train, alpha = 1, lambda = best_lambda_lasso)

# predict

lasso_preds_train <- predict(lasso_model, newx = X_train_mat)
lasso_preds_test  <- predict(lasso_model, newx = X_test_mat)

eval_metrics <- function(y_true, y_pred) {
  SSE <- sum((y_pred - y_true)^2)
  SST <- sum((y_true - mean(y_true))^2)
  
  r2 <- 1 - SSE / SST
  RMSE <- sqrt(mean((y_pred - y_true)^2))
  
  return(list(RMSE = RMSE, R2 = r2))
}

# Evaluate Lasso Model
lasso_train_results <- eval_metrics(Y_train, lasso_preds_train)
lasso_test_results <- eval_metrics(Y_test, lasso_preds_test)

print(paste("Lasso Regression - Train RMSE:", lasso_train_results$RMSE, "R²:", lasso_train_results$R2))
print(paste("Lasso Regression - Test RMSE:", lasso_test_results$RMSE, "R²:", lasso_test_results$R2))

```

## Lasso Coeficients

```{r}
lasso_coeffs <- coef(lasso_model)
lasso_df <- data.frame(Feature = rownames(lasso_coeffs), Coefficient = as.vector(lasso_coeffs))

# Show only important (nonzero) coefficients
lasso_df <- lasso_df[lasso_df$Coefficient != 0, ]
print(lasso_df)
```

## Lasso Model

```{r}
plot(Y_test, lasso_preds_test, main = "Actual vs. Predicted Prices (Lasso Regression)",
     xlab = "Actual Prices", ylab = "Predicted Prices", col = "blue", pch = 16)
abline(0, 1, col = "red", lwd = 2)
```

## Lasso Coefficients Shrinkage

```{r}
plot(lasso_model, xvar = "lambda", label = TRUE)
```

# Conclusion of which is the best model to implement

# Model Evaluation and Prediction

### F-test

```{r}
results <- anova(final_regression_model)
print(results)
```

The Analysis of Variance (ANOVA) table indicates that all the predictors included in the model are statistically significant in explaining the variability in the response variable, `ln_price`. Each predictor, such as `sqft_living`, `grade`, `bathrooms`, `view`, `condition`, and `yr_built`, has an extremely small p-value of `< 2.2e-16`, which strongly supports their significance. Among these, `sqft_living` and `grade` are particularly influential, as evidenced by their high Sum of Squares (1369.45 and 275.77, respectively) and substantial F-values (19242.814 and 3874.972, respectively). The other predictors, while less influential, still make meaningful contributions to the model, with factors like `view` and `condition` showing F-values of 379.824 and 992.788, respectively.

The residuals have a Sum of Squares of 1250.62 and a low Mean Square of 0.07, indicating that the variance not accounted for by the predictors is minimal. This suggests that the model fits the data well overall. The high F-values across all predictors reflect their strong ability to explain the variability in the response, further affirming the robustness of the model. In conclusion, the ANOVA results highlight that the chosen predictors collectively account for most of the variability in `ln_price`, making the model both statistically sound and effective for explaining the response variable.

### R-squared

```{r}
summary(final_regression_model)
```

In this model summary, the **Multiple R-squared value** is 0.6164, which indicates that approximately **61.64% of the variability in the response variable (`ln_price`)** is explained by the predictors included in the model (`sqft_living`, `grade`, `bathrooms`, `view`, `condition`, and `yr_built`).

The **Adjusted R-squared value** is **0.6163**, which is slightly lower than the Multiple R-squared. This adjusted value accounts for the number of predictors in the model relative to the sample size and helps prevent overestimation of the model's explanatory power when additional variables are included. Since the difference between the two values is very small, it suggests that the model is not overfitted, and all predictors contribute meaningfully to explaining the response.

In summary, the R-squared values indicate that the model has a good fit, explaining a significant portion of the variability in `ln_price`, while leaving around 38.36% of the variability unexplained, potentially due to other factors not included in the model or inherent randomness in the data.

### RMSE

```{r}
residuals_model <- residuals(model)

# Calculate RMSE
rmse_value <- sqrt(mean(residuals_model^2))
print(paste("RMSE: ", rmse_value))
```

The RMSE (Root Mean Square Error) value of **0.2667** indicates the average error in the predictions made by the model. Specifically, it means that, on average, the predicted values of `ln_price` deviate from the actual observed values by about **0.267** units in the logarithmic scale.

In practical terms, a lower RMSE value is better, as it reflects more accurate predictions. Considering this model's context, an RMSE of **0.2667** suggests that the model performs well and makes reasonably precise predictions.

### MSE

```{r}
mse_value <- mean(residuals_model^2)
print(paste("MSE: ", mse_value))
```

The Mean Squared Error (MSE) value of **0.0711** represents the average squared difference between the predicted and actual values of the response variable, `ln_price`. Since the MSE is the square of the Root Mean Square Error (RMSE), it captures the same concept but emphasizes larger errors more heavily due to squaring.

In this case, an MSE of **0.0711** suggests that, on average, the squared prediction errors are relatively small, which is a good sign for your model's accuracy.

### Normalization of the model comparison

```{r}

```

### Best Subset

```{r}
best_subset <- regsubsets(ln_price ~ sqft_living + bathrooms + grade + floors + view + condition + yr_built, data = cleaned_without_influential)


best_subset_summary <- summary(best_subset)


best_model_size <- which.max(best_subset_summary$adjr2)  # Model size with highest Adjusted R-Squared

# Extract the best set of predictors from the selected model
best_predictors <- names(coef(best_subset, best_model_size))[-1]  # Remove the intercept

# Create the formula for the final model with the selected predictors
formula <- as.formula(paste("ln_price ~", paste(best_predictors, collapse = " + ")))

# Fit the final linear model
final_model <- lm(formula, data = cleaned_without_influential)


cat("Model Summary:\n")
summary(final_model)


cat("\nVariance Inflation Factor (VIF):\n")
vif(final_model)
```

The best subset of predictors for the model was identified using the best subset selection method, which systematically evaluates all possible combinations of predictors to find the model that optimally balances goodness of fit and simplicity. This was achieved using the `regsubsets()` function, which computed models of varying sizes and provided a summary of evaluation metrics. The Adjusted R-squared value was chosen as the selection criterion to ensure the model explained the most variance in `ln_price` without unnecessary complexity.

The optimal model size, corresponding to the highest Adjusted R-squared value, was determined using `which.max(best_subset_summary$adjr2)`. The predictors from this model were extracted using the `coef()` function, ensuring that only variables with meaningful contributions to the response were included. Finally, a regression model was built dynamically using these predictors with the `lm()` function, resulting in an optimized model that balances explanatory power and parsimony. This rigorous selection process ensured that the final predictors were both statistically significant and practically relevant.

### Prediction

```{r}
# Generate predictions for the test dataset
predictions <- predict(final_model, newdata = X_test)

# Plot predicted vs. actual values
plot(Y_test, predictions, 
     xlab = "Actual ln_price", 
     ylab = "Predicted ln_price", 
     main = "Predicted vs. Actual Values",
     col = "blue", pch = 16)
abline(0, 1, col = "red")  # Add a reference line (y = x)


```
